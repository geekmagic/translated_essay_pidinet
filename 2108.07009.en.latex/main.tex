\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
%% \usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{booktabs}
\usepackage{hhline}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[breaklinks=true,bookmarks=false]{hyperref}



\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9187} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Pixel Difference Networks for Efficient Edge Detection}

\author{Zhuo Su$^{1,\ast}$\;\;\;\;\;
Wenzhe Liu$^{2,}$\thanks{Equal contributions.\;\;\;$\dagger$ Corresponding author: \href{http://lilyliliu.com}{http://lilyliliu.com}}\;\;\;\;\;
Zitong Yu$^1$\;\;\;\;\;
Dewen Hu$^2$\;\;\;\;\;
Qing Liao$^3$\;\;\;\;\;
Qi Tian$^4$\\
Matti Pietik{\"a}inen$^1$\;\;\;\;\;
Li Liu$^{2,1,\dagger}$\\
$^1$Center for Machine Vision and Signal Analysis, University of Oulu, Finland\\
$^2$National University of Defense Technology, China\\
$^3$Harbin Institute of Technology (Shenzhen), China\;\;\;\;\;$^4$Xidian University, China\\
{\tt\small \{zhuo.su, zitong.yu, matti.pietikainen, li.liu\}@oulu.fi}\\ 
{\tt\small \{liuwenzhe15, dwhu\}@nudt.edu.cn, liaoqing@hit.edu.cn, wywqtian@gmail.com}
}


%\author{Zhuo Su\\
%Institution1\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
%% For a paper whose authors are all at the same institution,
%% omit the following lines up until the closing ``}''.
%% Additional authors and addresses can be added with ``\and'',
%% just like the second author.
%% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
%}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
%\textcolor{blue}{

% Original
%Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level performance in edge detection due to their ability of learning rich and abstract edge representations from the training images automatically. However, the high performance of CNN based edge detection is achieved with a large pretrained CNN backbone, which is memory and energy consuming. In addition, it is surprising that the previous wisdom from the traditional edge detectors, such as Canny, Sobel, and LBP are rarely investigated in deep CNNs.

%Zitong's compact version
Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level performance in edge detection with the rich and abstract edge representation capacities. However, the high performance of CNN based edge detection is achieved with a large pretrained CNN backbone, which is memory and energy consuming. In addition, it is surprising that the previous wisdom from the traditional edge detectors, such as Canny, Sobel, and LBP are rarely investigated in 
%deep CNNs.
the rapid-developing deep learning era. 
To address these issues, 
%In this paper, 
we propose a simple, lightweight yet effective architecture named Pixel Difference Network (PiDiNet) for efficient edge detection. 
%\textcolor{red}{Li: Reprase this sentence as I do not quite understand it. Maybe a little more specific?? 
PiDiNet adopts novel pixel difference convolutions that integrate the traditional edge detection operators into the popular convolutional operations in modern CNNs for enhanced performance on the task, which enjoys the best of both worlds.
%} 
Extensive experiments on BSDS500, NYUD, and Multicue are provided to demonstrate its effectiveness, and its high training and inference efficiency. Surprisingly, when training from scratch with only the BSDS500 and VOC datasets, PiDiNet can surpass the recorded result of human perception (0.807 vs. 0.803 in ODS F-measure) on the BSDS500 dataset with 100 FPS and less than 1M parameters. A faster version of PiDiNet with less than 0.1M parameters can still achieve comparable performance among state of the arts
with 200 FPS.
%while running at 200 FPS. 
%\textcolor{magenta}{
Results on the NYUD and Multicue datasets show similar observations. 
%Codes are available at \url{https://github.com/zhuoinoulu/pidinet}.
The codes are available at  \href{https://github.com/zhuoinoulu/pidinet}{https://github.com/zhuoinoulu/pidinet}.

% 摘要太长了。。。删一句话好一点。。。


%Yu: I suggest to remove this sentences, and maybe add one sentence to describe your promising realtime real-world applications such as xxx.}}

%\textcolor{blue}{Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level performance in edge detection due to their ability of learning rich and abstract edge representations from the training images automatically and the high computational power of GPUs. However, it is surprising that the previous wisdom from the traditional detectors, such as Canny, Sobel, and LBP descriptors were ignored in front of the rapid development of CNNs. At the same time, top-performing CNN models for edge detection are still memory consuming and energy hungry, most of which are based on a large pretrained backbone.} This paper aims to solve the issues with: 1. A novel convolutional operation that integrates the traditional edge detection operators with convolution in modern CNNs for enhanced performance on the task; 2. A light-weight architecture family called PiDiNet that leads to both training efficiency and inference efficiency. PiDiNet can surpass the recorded result of human perception (0.807 vs. 0.803 in ODS F-measure) on the BSDS500 dataset with nearly 100 FPS and less than 1M parameters, which only uses the BSDS500 and voc datasets for training from scratch. A faster PiDiNet with 200 FPS and less than 100K parameters can still achieve comparable performance among state-of-the-arts. Experiments on NYUD and Multicue datasets further demonstrate our methods.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

%\textcolor{blue}{
Edge detection has been a longstanding, fundamental low-level problem in computer vision~\cite{canny1986computational}. Edges and object boundaries play an important role in various higher-level computer vision tasks such as object recognition and detection \cite{liu2020deep,ferrari2007groups}, object proposal generation \cite{cheng2014bing,uijlings2013selective}, image editing \cite{elder1998imageediting}, and image segmentation \cite{muthukrishnan2011edgeimageseg,bertasius2016semantic}. Therefore, recently, the edge detection problem has also been revisited and injected new vitality due to the renaissance of deep learning~\cite{bertasius2015deepedge,kokkinos2015deepboundary,shen2015deepcontour,xie2017holistically,wang2017ced,liu2019richer} .
%\textcolor{red}{\cite{???}Li: cite some recent deep edge methods}. \textcolor{red}{Li: too sudden to me...It is vitally important to implement edge detection efficiently and accurately, especially for real world applications. }
%}
%Edge detection has been a long-standing computer vision task, the process of which serves to simplify the analysis of images by drastically reducing the amount of data to be processed while at the same time preserving useful structural information about object boundaries~\cite{canny1986computational}, making it be usually considered as a low-level technique, benefiting other mid- or high-level tasks such as object detection [][], object proposal [][] and image segmentation [][]. Therefore, it is vitally important to implement edge detection efficiently and accurately, especially for real life applications.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{images/fig1_rebuttal.pdf}
    \caption{PDC benefits from both worlds with proper integration of traditional operators and modern CNNs.}
    \label{fig:figure1}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{images/figure_intro2.pdf}
    \caption{PiDiNet configured with pixel difference convolution (PDC) \emph{vs.} the baseline with vanilla convolution. Both models were trained only using the BSDS500 dataset. Compared with vanilla convolution, PDC can better capture gradient information from the image that facilitates edge detection.}
    \label{fig:figure2}
\end{figure*}

%\textcolor{blue}{
The main goal of edge detection is identifying sharp image brightness changes such as discontinuities in intensity, color, or texture~\cite{torre1986edge}. Traditionally, edge detectors based on image gradients or derivatives information are popular choices.
%} 
Early classical methods use the first or second order derivatives (\emph{e.g.}, Sobel~\cite{sobel19683x3}, Prewitt~\cite{prewitt1970object}, Laplacian of Gaussian (LoG), Canny~\cite{canny1986computational}, \emph{etc.}) for basic edge detection. Later learning based methods~\cite{hallman2015oef,dollar2014se} further utilize various gradient information~\cite{xiaofeng2012scg, martin2004pb,fowlkes2002learningpb,gupta2014seng} to 
%include the ones using the first order derivatives, such as Sobel operator~\cite{sobel19683x3}, Prewitt operator~\cite{prewitt1970object}, Roberts cross operator~\cite{roberts1963machine}, and the ones adopting the second order derivatives, like Laplacian of Gaussian (LoG) and Difference of Gaussians (DoG)~\cite{davidson2006differenceofgaussain}. 
%\textcolor{blue}{
%There were also learning based methods
%utilizing image features based on various gradient information to 
%that can effectively eliminate the background noise hidden in the gradient information and 
produce more accurate boundaries.
%~\cite{martin2004pb,xiaofeng2012scg,dollar2014se,hallman2015oef}.
%}For example, the work in \textcolor{red}{\cite{???}} created edge maps with \textcolor{red}{ODS=.xxx, substantially beating the classical Canny detector~\cite{canny1986computational}, whose xxx was only xx, and was approaching the human-level performance. Not finished sentence???} 


%\textcolor{blue}{
Due to the capability of automatically learning rich representations of
data with hierarchical levels of abstraction, deep CNNs have brought tremendous progress for various computer vision tasks including edge detection and are still rapidly developing. Early deep learning based edge detection models
construct CNN architectures as classifiers to predict the edge probability of an input image patch~\cite{bertasius2015deepedge,shen2015deepcontour,bertasius2015hfl}.  
%\textcolor{red}{Li: I do not understand this sentence followed, please rephrase???} utilize the character that deep CNNs can learn semantically meaningful features to output the ``edgeness'' of an input pixel [][] or patch [][]. \textcolor{red}{???What do you mean by this?Rephrase? Later research further found that the deep CNNs are naturally more versatile.} For example, by interpreting typical deep CNNs as Fully Convolutional Networks (FCNs) and leveraging both the coarse from later and the fine representations from earlier layers, FCNs can efficiently perform end to end semantic segmentation~\cite{long2015fully}. 
Building on top of fully convolutional networks~\cite{long2015fully}, HED~\cite{xie2017holistically} performs end-to-end edge detection by leveraging multilevel image features with rich hierarchical information guided by deep supervision, and achieves state-of-the-art performance. Other similar works include~\cite{yang2016cedn,kokkinos2015deepboundary,maninis2016cob,wang2017ced,xu2018amhnet,liu2019richer,deng2018lpcb,he2019bidirectional}.

%Since HED, many edge detection based on deep learning have been proposed [][][].
%}

%Beginning with its huge success on image classification [], CNN models have been extended to other computer vision tasks, among which edge detection was also the beneficiary. Inspired by image classification, early deep learning based edge detection models utilize the the character that deep CNNs can learn semantically meaningful features, to output the ``edgeness'' of an input pixel [][] or patch [][]. Later research further found that the deep CNNs are naturally more versatile. For example, by reinterpreting classification nets as fully convolutional and leveraging both the coarse and fine representations from different depths in the primary network, the fully convolutional network (FCN) can efficiently do end-to-end pixel wise prediction for semantic segmentation~\cite{long2015fully}. Based on FCN, HED~\cite{xie2017holistically} combined multi-scale and multi-level image features with rich hierarchical information guided by deep supervision, leading to efficient and accurate end-to-end edge prediction, and inspiring many related works in the edge detection literature [][][]. 


%However, it seems like there is no obvious intersection between the traditional edge detection algorithms and the modern CNN based approaches, resulting in a development ``fracture'' in the edge detection history. Previous wisdom accumulated by numerous traditional methods as mentioned above, is drowning out with the rapid evolution of deep learning. 
However, integration of traditional edge detectors with modern CNNs were rarely investigated. The former were merely utilized as auxiliary tools to extract candidate edge points in some prior approaches~\cite{bertasius2015hfl,bertasius2015deepedge}.
Intuitively, edges manifest diverse specific patterns like straight lines, corners, and ``X'' junctions. On one hand, traditional edge operators like those shown in Fig. 1 are inspired by these intuitions, and based on gradient computing which encodes important gradient information for edge detection by explicitly calculating pixel differences. However, these handcrafted edge operators or learning based edge detection algorithms are usually not powerful enough due to their shallow structures. On the other hand, modern CNNs can learn rich and hierarchical image representations, where vanilla CNN kernels serve as probing local image patterns. Nevertheless, CNN kernels are optimized by starting from random initialization which has no explicit encoding for gradient information, making them hard to focus on edge related features.

%By observing the kernels of the traditional edge detection operators and the convolution kernels of a CNN based edge detection model (our baseline in Section~\ref{sec:experiments}), as shown in Fig.~\ref{fig:figure1}, we found they have a big mismatch in the kernel formations. When traditional operators convolve with an input image, there is a process where the value differences among pixels covered by the kernel are calculated, thereby rich gradient information related to edge patterns can be gathered. Although the convolution kernels in deep CNNs can learn this similar character, the learning process can be hard or infeasible, as during the training, from the initialization state to the final state, the weights in the convolution kernels nearly conform to a Gaussian distribution, making the convolution prone to smoothing the covered pixels, which may instead erase much edge information. 

\begin{table}[t!]
\caption{Comparison between ours and some leading edge detection models in terms of efficiency and accuracy. The multiply-accumulates (MACs) are calculated based on a 200$\times$200 image, FPS and ODS \emph{F-measure} are evaluated on the BSDS500 test set.}
%\vspace{1.3em}
\begin{center}
\setlength{\tabcolsep}{0.015\linewidth}
\resizebox*{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule[1pt]
 & \emph{HED}~\cite{xie2017holistically} & \emph{RCF}~\cite{liu2019richer} & \emph{BDCN}~\cite{he2019bidirectional} & \emph{PiDiNet} & \emph{PiDiNet(tiny)} \\
\hline
Params & 14.7M & 14.8M & 16.3M & 710K & 73K \\
MACs & 22.2G & 16.2G & 23.2G & 3.43G & 270M \\
Throughput & 78FPS & 67FPS & 47FPS & 92FPS & 215FPS\\
Pre-training & ImageNet & ImageNet & ImageNet & No & No \\
ODS \emph{F-measure} & 0.788 & 0.806 & 0.820 & 0.807 & 0.787 \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\label{table:table1}
%\vspace{1.3em}
\vspace{-0.2in}
\end{table}

We believe a new type of convolutional operation can be derived, to satisfy the following needs. Firstly, it can easily capture the image gradient information that facilitates edge detection, and the CNN model can be more focused with the release of burden on dealing with much unrelated image features. Secondly, the powerful learning ability of deep CNNs can still be preserved, to extract semantically meaningful representations, which lead to robust and accurate edge detection. In this paper, we propose pixel difference convolution (PDC), where the pixel differences in the image are firstly computed, and then convolved with the kernel weights to generate output features (see Fig.~\ref{fig:pdc}). We show PDC can effectively improve the quality of the output edge maps, as illustrated
%It is hoped that with the proposed convolution, the ``fracture'' can be to some extent filled, as shown 
in Fig.~\ref{fig:figure2}. 

On the other hand, leading CNN based edge detectors suffer from the deficiencies as shown in Table~\ref{table:table1}: being memory consuming with big model size, being energy hungry with high computational cost, running inefficiency with low throughput and label inefficiency with the need of model pre-training on large scale dataset. 
This is due to the fact that the annotated data available for training edge detection models is limited, and thus a well pretrained (usually large) backbone is needed.
%relatively small (\emph{i.e.}, the BSDS500~\cite{arbelaez2010bsds} has only 200 image samples in the official training set). 
%Therefore, they are usually obtained by fine-tuning from a well-trained (usually large) backbone. 
For example, the widely adopted routine is to use the large VGG16~\cite{simonyan2014very} architecture that was trained on the large scale ImageNet dataset~\cite{deng2009imagenet}. 
%, including 1.2 million training samples. 

%we believe it is both necessary and feasible to solve the mentioned issues in one time by building an architecture with small model size, high running efficiency and can be trained from scratch using limited datasets for effective edge detection.

%To enable fast edge detection, which is particularly vital when edge detection is considered as a pre-processing technique for real life applications, 
It is important to develop a lightweight structure, to achieve a better trade-off between accuracy and efficiency for edge detection. With pixel difference convolution, inspired by~\cite{he2016residual,howard2017mobilenets}, we build a new end-to-end architecture, namely Pixel Difference Network (PiDiNet) to solve the mentioned issues in one time. Specifically, PiDiNet consists of an efficient backbone and an efficient task-specific side structure (see Fig.~\ref{fig:arch}), able to do robust and accurate edge detection with high efficiency.

%In conclusion, the contribution in this paper is three-fold:
%Firstly, we derive the pixel difference convolution which integrates the wisdom from the traditional edge detectors and the advantages of the deep CNNs, leading to more accurate edge detection.
%Secondly, we propose a highly efficient architecture family called PiDiNet, which are memory friendly and with high inference speed. Furthermore, PiDiNet can be trained from scratch only using limited data samples, while achieving human-level performances, breaking the convention that high performance CNN based edge detectors usually need a backbone pretrained on large scale dataset (\emph{e.g.}, ImageNet~\cite{deng2009imagenet}).
%Thirdly, we conduct extensive experiments on BSDS500, NYUD and Multicue datasets for edge detection. We believe the PiDiNet creates new benchmarks of state-of-the-arts considering both accuracy and efficiency (see Fig.~\ref{fig:efficient}). 


\section{Related Work}

%Since we have discussed common approaches for edge detection in Section~\ref{sec:intro}, here we only focus on the most related works from other points.

%\vspace{0.3em}
\noindent \textbf{Using Traditional Edge Detectors to Help Deep CNN Models for Edge Detection.} \quad Canny~\cite{canny1986computational} and SE~\cite{dollar2014se} edge detectors are usually used to extract candidate contour points before applying the CNN model for contour/non-contour prediction~\cite{bertasius2015deepedge, bertasius2015hfl}. The candidate points can be also used as auxiliary relaxed labels for better training the CNN model~\cite{liu2016relaxed}. Instead of relying on the edge information from the hand-crafted detectors, PDC directly integrates the gradient information extraction process into the convolutional operation, which is more compact and learnable.

\vspace{0.3em}
\noindent \textbf{Lightweight Architectures for Edge Detection.} \quad Recently, efforts have been made to design lightweight architectures for efficient edge detection~\cite{wibisono2020fined,wibisono2020traditional,poma2020dense}. Some of them may not need a pretrained network based on large scale dataset~\cite{poma2020dense}. Although being compact and fast, the detection accuracies with these networks are unsatisfactory. Alternatively, lightweight architectures for other dense prediction tasks~\cite{gao2020100k,wu2020cgnet,paszke2016enet,li2019dabnet,mehta2019espnetv2,yu2018bisenet} and multi-task learning~\cite{kokkinos2017ubernet, liu2020dynamicintegration} may also benefit edge detection. However, the introduced sophisticated multi-branch based structures may lead to running inefficiency. Instead, we build a backbone structure which only uses a simple shortcut~\cite{he2016residual} as the second branch for the convolutional blocks.

\vspace{0.3em}
\noindent \textbf{Integrating Traditional Operators.}  \quad The proposed PDC is mostly related to the recent central difference convolution (CDC)~\cite{yu2020cdc,yu2020fas,yu2021dual,yu2021searching} and local binary convolution (LBC)~\cite{juefei2017lbc}, of which both derive from local binary patterns (LBP)~\cite{ojala2002lbp} and involve calculating pixel differences during convolution. LBC uses a set of predefined sparse binary filters to generalize the traditional LBP, focusing on reducing the network complexity. CDC further proposes to use learnable weights to capture image gradient information for robust face anti-spoofing. CDC can be seen as one instantiated case of the proposed PDC (\emph{i.e.}, Central PDC), where the central direction is considered, as we will introduce in Section~\ref{sec:pdc}. Like CDC, PDC uses learnable filters while being more general and flexible to capture rich gradient information for edge detection. On the other hand, Gabor convolution~\cite{luan2018gabor} encodes the orientation and scale information in the convolution kernels by multiplying the kernels with a group of Gabor filters, while PDC is more compact without any auxiliary traditional feature filters.

%\vspace{0.3em}
%\noindent  \textbf{Lightweight Architectures for Other Dense Prediction Tasks.} \quad  Lightweight architectures for other dense prediction tasks~\cite{gao2020100k,wu2020cgnet,paszke2016enet,li2019dabnet,mehta2019espnetv2,yu2018bisenet} and multi-task learning~\cite{kokkinos2017ubernet, liu2020dynamicintegration} may also benefit edge detection. However, the introduced sophisticated multi-branch based structures may lead to running inefficiency. Instead, we build a backbone structure which only uses a simple shortcut~\cite{he2016residual} as the second branch for the convolutional blocks.


%Approaches for dense prediction tasks, such as semantic segmentation, salient object detection, and edge detection may share the similar network architectures. Besides, multi-task learning is popular~\cite{kokkinos2017ubernet, liu2020dynamicintegration}. Although lightweight architectures for other tasks~\cite{gao2020100k,wu2020cgnet,paszke2016enet,li2019dabnet,mehta2019espnetv2,yu2018bisenet} may also benefit edge detection, the sophisticated multi-branch based structures may lead to running inefficiency. Instead, we build a backbone structure which only uses a simple shortcut~\cite{he2016residual} as the second branch for the convolutional blocks.


\section{Pixel Difference Convolution}
\label{sec:pdc}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/pdc.pdf}
    \caption{Three instances of pixel difference convolution derived from extended LBP descriptors~\cite{liu2011sorted, liu2012extended, su2019bird}. One can derive other instances by designing the picking strategy of the pixel pairs.}
    \label{fig:pdc}
\end{figure}

The process of pixel difference convolution (PDC) is pretty similar to that of vanilla convolution,
%\textcolor{magenta}{conventional convolution Yu: I suggest to use the unified phrase `vanilla convolution' for the whole manuscript, same for Fig.2 and so on}, 
where the original pixels in the local feature map patch covered by the convolution kernels are replaced by pixel differences, when conducting the convolutional operation. The formulations of vanilla convolution and PDC can be written as:
%Therefore, PDC can be in any form, as long as there are certain number of pixel pairs in the local patch, for pixel intensity comparison. A general formulation of PDC can be written as:
\begin{align}
    y &= f(\pmb{x}, \pmb{\theta}) = \sum_{i=1}^{k\times k}w_{i}\cdot x_{i}, \;\;\;\;\;\;\; \text{(vanilla convolution)} \\
    y &= f(\triangledown\pmb{x}, \pmb{\theta}) = \sum_{(x_i, x_i')\in \pmb{\mathcal{P}}}w_{i}\cdot (x_i - x_i'), \;\;\;\;\;\;\, \text{(PDC)} \label{eq: pdc}
\end{align}
where, $x_i$ and $x_i'$ are the input pixels, $w_i$ is the weight in the $k \times k$ convolution kernel. $\pmb{\mathcal{P}} = \{(x_1, x_1'), (x_2, x_2'), ..., (x_m, x_m')\}$ is the set of pixel pairs picked from the current local patch, and $m\le k\times k$. 

To capture rich gradient information, the pixel pairs can be selected according to different strategies, which can be inspired from the numerous traditional feature descriptors. Here, we utilize the ideas from the work in~\cite{ojala2002lbp,liu2012extended,su2019bird}, where the local binary pattern (LBP) and its robust variants, extended LBP (ELBP), were used to encode pixel relations from varying directions (angular and radial). Specifically, ELBP are obtained by firstly calculating the pixel differences within a local patch (from $m$ pixel pairs), resulting in a pixel difference vector, and then binarizing the vector to create an $m$-length 0/1 code. Then, the bag-of-words technique~\cite{liu2019bow} is usually used to calculate the code distribution (or histogram), which is regarded as the image representation. In ELBP, the angular and radial directions were will demonstrated to help encode potential discriminative image cues and be complementary for increasing the feature representational capacity for various computer vision tasks, such as texture classification~\cite{liu2012extended,liu2011sorted} and face recognition~\cite{su2019bird}.

By integrating ELBP with CNN convolution, we derive three types of PDC instances as shown in Fig.~\ref{fig:pdc}, in which we name them as central PDC (CPDC), angular PDC (APDC) and radial PDC (RPDC) respectively. The pixel pairs in the local patch is easy to understand. For example, for the APDC with kernel size $3\times 3$, we create 8 pairs in the angular direction in the $3\times 3$ local patch (thus $m=8$), then the pixel differences obtained from the pairs are convolved with the kernel by doing an element-wise multiplication with the kernel weights, followed by a summation, to generate the value in the output feature map.
%generating the output value for the feature map in the next layer. 

The derived PDC instances based on ELBP can be seen as an extension of ELBP that are more flexible and learnable. Although being powerful, the original ELBP codes are discrete with limited representative ability. While the useful encodings of pixel relations in PDC will be preserved in the trained convolution kernels, as during the training process of CNN, the convolution kernels will be encouraged to have higher 
%cosine similarity 
inner product
%(usually indicating a higher cosine similarity)
with those important encodings, in order to create higher activation responses\footnote{Usually, higher activation responses are considered to be more salient, as adopted in many network pruning methods~\cite{han2015deepcompression,su2020dynamic}}. By training from abundant of data, PDC is able to automatically learn rich representative encodings for the task. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/APDC.pdf}
    \caption{Selection of pixel pairs and convolution in APDC.}
    \label{fig:apdc}
\end{figure}

\vspace{0.3em}
\noindent \textbf{Converting PDC to Vanilla Convolution.} \quad
According to Eq.~\ref{eq: pdc}, one may notice that the computational cost and memory footprint by PDC are doubled compared with the vanilla counterpart. However, once the convolution kernels have been learnt, PDC layers can be converted to vanilla convolutional layers by instead saving the differences of the kernel weights in the model, according to the locations of the selected pixel pairs. In this way, the efficiency is maintained during inference. Taking APDC as an example (Fig.~\ref{fig:apdc}), conversion is done with the following equations:
{\small 
\begin{align}
    y &= w_{1}\cdot (x_1 - x_2) + w_2\cdot (x_2 - x_3)+w_3\cdot (x_3 - x_6) + ... \nonumber\\
    &=(w_1 - w_4)\cdot x_1 + (w_2 - w_1)\cdot x_2 + (w_3 - w_2)\cdot x_3 + ...\nonumber\\
    &=\hat{w}_1\cdot x_1 + \hat{w}_2\cdot x_2 + \hat{w}_3\cdot x_3 + ... =\sum \hat{w}_i\cdot x_i.
\end{align}
}

It is worth mentioning that we can also use this tweak to speed up the training process, where the differences of kernel weights are firstly calculated, followed by the convolution with the untouched input feature maps.
We have illustrated more details in the appendix.
%A more detailed illustration about the converting process can be seen in the appendix.


\section{PiDiNet Architecture}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{images/arch.pdf}
    \caption{PiDiNet architecture.}
    \label{fig:arch}
\end{figure*}

%Inheriting the algorithm from the holistically-nested edge detection (HED) network in [], our architecture is also expected to be an end-to-end dense prediction model which can learn multi-level and multi-scale hierarchical image representations with deep supervision. Specifically, the proposed model will be made up of a backbone as the primary network to extract fine and coarse information, and a side structure specific to the pixel-wise edge detection task that leverages features in multi-stages from the backbone guided by supervision directly from the ground truth. However, the HED network and those in the derived approaches [][] suffer from the deficiencies as shown in Table~\ref{table:table1}: memory consuming with big model size, energy hungry with high computational cost, running inefficiency with low throughput and label consuming with the need of model pre-training on large scale dataset. 

%This is due to the fact that the annotated data available for training edge detection models is relatively small (\emph{i.e.}, the BSDS500~\cite{arbelaez2010bsds} has only 200 image samples in the official training set). Therefore, a well-trained while usually large backbone which can already produce rich coarse and fine image representations, is utilized followed by a fine-tuning process for the specific edge detection task. For example, the widely adopted solution is using the first stages of the large VGG16~\cite{simonyan2014very} architecture that was trained on the large scale ImageNet dataset~\cite{deng2009imagenet}, including 1.2 million training samples. 

%\begin{table}[t!]
%\caption{fps and params, macs calculated based on a 200x200 image, ods on bsdsd500}
%%\vspace{-0.1in}
%\begin{center}
%\setlength{\tabcolsep}{0.015\linewidth}
%\resizebox*{\linewidth}{!}{
%\begin{tabular}{lccccc}
%\toprule[1pt]
%% & Linear & XNOR-Net & P \RN{1} & P \RN{1} + \RN{2} \\
% & \emph{HED}~\cite{xie2017holistically} & \emph{RCF}~\cite{liu2019richer} & \emph{BDCN}~\cite{he2019bidirectional} & \emph{PiDiNet} & \emph{PiDiNet(tiny)} \\
%\hline
%Params & 14.7M & 14.8M & 16.3M & 710K & 73K \\
%MACs & 22.2G & 16.2G & 23.2G & 3.43G & 270M \\
%Throughput & 78FPS & 67FPS & 47FPS & 92FPS & 215FPS\\
%Pre-training & ImageNet & ImageNet & ImageNet & No & No \\
%ODS-\emph{F-measure} & 0.788 & 0.806 & 0.820 & 0.807 & 0.787 \\
%\bottomrule[1pt]
%\end{tabular}
%}
%\end{center}
%\label{table:table1}
%%\vspace{-0.3in}
%\end{table}

As tried by some prior works~\cite{wibisono2020fined,poma2020dense,wibisono2020traditional}, we believe it is both necessary and feasible to solve the inefficiency issues mentioned in Section~\ref{sec:intro} in one time by building an architecture with small model size and high running efficiency, and can be trained from scratch using limited datasets for effective edge detection. We construct our architecture with the following parts (Fig.~\ref{fig:arch}).

\vspace{0.3em}
\noindent \textbf{Efficient Backbone.} \quad The building principle for the backbone is to make the structure slim while own high running efficiency. Thus we do not consider the sophisticated multi-branch lightweight structures proposed for many other tasks~\cite{gao2020100k,mehta2019espnetv2,yu2018bisenet}, since they may not appeal to parallel implementation~\cite{ma2018shufflenetv2}, leading to unsatisfactory efficiency for the edge detection task. Inspired from \cite{he2016residual} and \cite{howard2017mobilenets}, we use the separable depth-wise convolutional structure with a shortcut for fast inference and easy training. The whole backbone has 4 stages
and max pooling layers are among them for down sampling.
%connected by the max pooling layers with stride of 2 for down sampling, and 
Each stage has 4 residual blocks (except the first stage that has an initial convolutional layer and 3 residual blocks). The residual path in each block includes a depth-wise convolutional layer, a ReLU layer, and a point-wise convolutional layer sequentially. 
%, where we do not change the shape of output for all the layers.
The number of channels in each stage is reasonably small to avoid big model size ($C$, $2\times C$, $4\times C$ and $4\times C$ channels for stage 1, 2, 3, and 4 respectively).
%To avoid the network being fat, we control the number of channels to be reasonably small, and double that in the next stage except the last stage (\emph{i.e.}, $C$, $2\times C$, $4\times C$ and $4\times C$ channels for stage 1, 2, 3, and 4 respectively). 
%Specifically, we double the number of channels in the point-wise convolutional layer and adopt a 1$\times$1 convolutional shortcut to match the channel number in the first block of stage 2 and 3.

\vspace{0.3em}
\noindent \textbf{Efficient Side Structure.} \quad %Inspired by~\cite{xie2017holistically},  
To learn rich hierarchical edge representation, we also use the side structure as in~\cite{xie2017holistically} to generate an edge map from each stage respectively, based on which a side loss is computed with the ground truth map to provide deep supervision~\cite{xie2017holistically}. 
%Inspired from many related works [][], 
To refine the feature maps, beginning from the end of each stage, we firstly build a compact dilation convolution based module (CDCM) to enrich multi-scale edge information, which takes the input with $n\times C$ channels, and produces $M$ ($M < C$) channels in the output to relieve the computation overhead, followed by a compact spatial attention module (CSAM) to eliminate the background noise. After that, a $1\times 1$ convolutional layer further reduces the feature volume to a single channel map, which is then interpolated to the original size followed by a Sigmoid function to create the edge map. The final edge map, which is used for testing, is created by fusing the 4 single channel feature maps with a concatenation, a convolutional layer and a Sigmoid function. 

The detailed structure information can be seen in Fig.~\ref{fig:arch}, noting that we do not use any normalization layers for simplicity since the resolutions of the training images are not uniform. The obtained architecture is our baseline. By replacing the vanilla convolution in the 
$3\times 3$ depth-wise convolutional layer in the residual blocks with PDC, we get the proposed PiDiNet. %, which is highly efficient and effective as shown in Fig.~\ref{fig:efficient}. % Table~\ref{table:table1}.

\vspace{0.3em}
\noindent \textbf{Loss Function.} \quad We adopt the annotator-robust loss function proposed in~\cite{liu2019richer} for each generated edge map (including the final edge map). For the $i$th pixel in the $j$th edge map with value $p_i^j$, the loss is calculated as:
%$l_i^j = \{\alpha\cdot \log (1 - p_i^j)\;\; \text{if } y_i  = 0;\;\; 0\;\; \text{if } 0 < y_i < \eta;\;\; \beta\cdot \log p_i^j\;\; \text{otherwise}\}$,
{\small \begin{equation}
    l_i^j = \begin{cases} 
    \alpha\cdot \log (1 - p_i^j) &\text{if } y_i  = 0 \\
    0 &\text{if } 0 < y_i < \eta \\
    \beta\cdot \log p_i^j &\text{otherwise},
    \end{cases}
\end{equation}}
where $y_i$ is the ground truth edge probability, $\eta$ is a pre-defined threshold, meaning that a pixel is discarded and not considered to be a sample when calculating the loss if it is marked as positive by fewer than $\eta$ of annotators to avoid confusing, $\beta$ is the percentage of negative pixel samples and $\alpha = \lambda\cdot (1-\beta)$. After all, the total loss is $L = \sum_{i,j}l_i^j$.

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets and Implementation}
\noindent \textbf{Experimental Datasets.} \quad We evaluate the proposed PiDiNet on three widely used datasets, namely, BSDS500~\cite{arbelaez2010bsds}, NYUD~\cite{shi2000nyud}, and Multicue~\cite{mely2016multicue}. The experimental settings about data augmentation and configuration on the three datasets follow~\cite{xie2017holistically,liu2019richer,he2019bidirectional} and the details are given below. BSDS500 consists of 200, 100, and 200 images in the training set, validation set, and test set respectively. Each image has 4 to 9 annotators. 
%We use training and validation set for training, and test set for evaluation. Each training image is 
Training images in the dataset are augmented with flipping (2$\times$), scaling (3$\times$), and rotation (16$\times$), leading to a training set that is 96$\times$ larger than the unaugmented version. 
%When evaluating on BSDS500 dataset, 
Like prior works~\cite{xie2017holistically,liu2019richer,he2019bidirectional},
the PASCAL VOC Context dataset~\cite{mottaghi2014voc}, which has 10K labeled images (and augmented to 20K with flipping), is also optionally considered in training. NYUD has 1449 pairs of aligned RGB and depth images which are densely labeled. There are 381, 414 and 654 images for training, validation, and test respectively. We combine the training and validation set and augment them with flipping (2$\times$), scaling (3$\times$), and rotation (4$\times$) to produce the training data. Multicue is composed of 100 challenging natural scenes and each scene contains a left- and right-view color sequences captured by a binocular stereo camera. The last frame of left-view sequences for each scene, which is labeled with edges and boundaries, is used in our experiments. We randomly split them to 80 and 20 images for training and evaluation respectively. The process is independently repeated twice more. The metrics are then recorded from the three runs. We also augment each training image with flipping (2$\times$), scaling (3$\times$), and rotation ($16\times$), then randomly crop them with size 500$\times$500. 


\vspace{0.3em}
\noindent \textbf{Performance Metrics.} \quad During evaluation, \emph{F-measure} at both Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS) are recorded for all datasets. Since efficiency is one of the main focuses in this paper, all the models are compared based on the evaluations from single scale images if not specified.


\vspace{0.3em}
\noindent \textbf{Implementation Details.} \quad Our implementation is based on the Pytorch library~\cite{paszke2019pytorch}. In detail, PiDiNet (and the baseline) is randomly initialized and trained for 14 epochs with Adam optimizer~\cite{kingma2014adam} with an initial learning rate 0.005, which is decayed in a multi-step way (at epoch 8 and 12 with decaying rate 0.1). If VOC dataset is used in training for evaluating BSDS500, we train 20 epochs and decay the learning rate at epoch 10 and 16. 
%When calculating the loss, 
$\lambda$ is set to 1.1 for both BSDS500 and Multicue, and 1.3 for NYUD. The threshold $\eta$ is set to 0.3 for both BSDS500 and Multicue. 
%There is no need to set $\eta$ for NYUD 
No $\eta$ is needed for NYUD
since the images are singly annotated. 

\begin{table}[t!]
\caption{Possible configurations of PiDiNet. `C', `A', `R' and `V' indicate CPDC, APDC, RPDC and vanilla convolution respectively. `$\times$n' means repeating the pattern for $n$ times sequentially. For example, the baseline architectrue can be presented as ``[V]$\times$16'', and `C-[V]$\times$15' means using CPDC in the first block and vanilla convolutions in the later blocks. All the models are trained using BSDS500 training set and the VOC dataset, then evaluated on BSDS500 validation set.}
%\vspace{-0.1in}
\begin{center}
\setlength{\tabcolsep}{0.05\linewidth}
\resizebox*{\linewidth}{!}{
\begin{tabular}{l||l|l|l}
\toprule[1pt]
Architecture & C-[V]$\times$15 & A-[V]$\times$15 & R-[V]$\times$15 \\
\hline
ODS / OIS & 0.775 / 0.794 & 0.774 / 0.794 & 0.774 / 0.792 \\
\hline
Architecture & [CVVV]$\times$4 & [AVVV]$\times$4 & [RVVV]$\times$4 \\
\hline
ODS / OIS & 0.773 / 0.792 & 0.771 / 0.790 & 0.772 / 0.791 \\
\hline
Architecture & [CCCV]$\times$4 & [AAAV]$\times$4 & [RRRV]$\times$4 \\
\hline
ODS / OIS & 0.772 / 0.791 & 0.775 / 0.793 & 0.771 / 0.787 \\
\hline
Architecture & [C]$\times$16 & [A]$\times$16 & [R]$\times$16 \\
\hline
ODS / OIS & 0.767 / 0.786 & 0.768 / 0.786 & 0.758 / 0.777 \\
\hline
Architecture & Baseline & \multicolumn{2}{l}{\textbf{[CARV]$\times$4 (PiDiNet)}} \\
\hline
ODS / OIS & 0.772 / 0.792 & \multicolumn{2}{l}{\textbf{0.776 / 0.795}} \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\label{table:configuration}
%\vspace{-0.3in}
\end{table}

\begin{table}[t!]
\caption{More comparisons between PiDiNet and the baseline architecture in multiple network scales by changing the nubmer of channels $C$ (see Fig.~\ref{fig:arch}). The models are trained using the BSDS500 training set, and evaluated on BSDS500 validation set.}
%\vspace{-0.1in}
\begin{center}
\setlength{\tabcolsep}{0.05\linewidth}
\resizebox*{\linewidth}{!}{
\begin{tabular}{l|c|c}
\toprule[1pt]
Scale & Baseline (ODS / OIS) & PiDiNet (ODS / OIS) \\
\hline
Tiny (C=20) & 0.735 / 0.752 & \textbf{0.747 / 0.764} \\
\hline
Small (C=30) & 0.738 / 0.759 & \textbf{0.752 / 0.769} \\
\hline
Normal (C=60) & 0.736 / 0.751 & \textbf{0.757 / 0.776} \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\label{table:morecomparison}
%\vspace{-0.3in}
\end{table}

\begin{table}[t!]
\caption{Ablation on CDCM, CSAM and shortcuts. The models are trained with BSDS500 training set and VOC dataset, and evaluated on BSDS500 validation set.}
%\vspace{-0.1in}
\begin{center}
\setlength{\tabcolsep}{0.08\linewidth}
\resizebox*{\linewidth}{!}{
\begin{tabular}{ccc|c}
\toprule[1pt]
CSAM & CDCM & Shortcuts & ODS / OIS \\
\hline
\xmark & \xmark & \cmark & 0.770 / 0.790 \\
\hline
\xmark & \cmark & \cmark & 0.775 / 0.793 \\
\hline
\cmark & \cmark & \cmark & \textbf{0.776 / 0.795} \\
\hline
\cmark & \cmark & \xmark & 0.734 / 0.755 \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\label{table:moreablation}
%\vspace{-0.3in}
\end{table}

\subsection{Ablation Study}
To demonstrate the effectiveness of PDC and to find the possibly optimal architecture configuration, we conduct our ablation study on the BSDS500 dataset, where we use the data augmented from the 200 images in the training set (optionally mixed with the VOC dataset) for training and record the metrics on the validation set. 

\vspace{0.3em}
\noindent \textbf{Architecture Configuration.} \quad We can replace the vanilla convolution with PDC in any block (we also regard the initial convolutional layer as a block in the context) in the backbone. Since there are 16 blocks, and a brute force search for the architecture configurations is not feasible, hence we only sample some of them as shown in Table~\ref{table:configuration} by gradually increasing the number of PDCs. We found replacing the vanilla convolution with PDC only in a single block can even have obvious improvement. More replacements with the same type of PDC may no longer give extra performance gain and instead degenerate the model. We conjecture that the PDC in the first block already obtains much gradient information from the raw image, and an abuse of PDC may even cause the model fail to preserve useful information. The extreme case is that when all the blocks are configured with PDC, the performance becomes worse than that of the baseline. The best configuration is `[CARV]$\times$4', which means combing the 4 types of convolutions sequentially in each stage, as different types of PDC capture the gradient information in different encoding directions. We will use this configuration in the following experiments. 

To further demonstrate the superiority of PiDiNet over the baseline, which only uses the vanilla convolution, we give more comparisons as shown in Table~\ref{table:morecomparison}. It constantly proves that PDC configured architectures outperform the corresponding vanilla convolution configured architectures.

\vspace{0.3em}
\noindent \textbf{CSAM, CDCM and Shortcuts.} \quad The effectiveness of CSAM, CDCM and residual structures are demonstrated in Table~\ref{table:moreablation}. The addition of shortcuts is simple yet important, as they can help preserve the gradient information captured by the previous layers. On the other hand, the attention mechanism in CSAM and dilation convolution in CDCM can give extra performance gains, while may also bring some computational cost. Therefore, they can be used to tradeoff between accuracy and efficiency. In the following experiments, we note PiDiNet without CSAM and CDCM as PiDiNet-L (meaning a more lightweight version).

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/ex_1.pdf}
    \caption{Exploration on the scalability of PiDiNet. The structure sizes are changed by slimming or widening the basic PiDiNet. Bottom row shows the number of parameters for each model. The models are trained with or without VOC dataset.}
    \label{fig:scalability}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/fps_param.pdf}
    \caption{Comparison with other methods in terms of network complexity, running efficiency and detection performance (on BSDS500 dataset). The running speeds of FINED~\cite{wibisono2020fined} are cited from the original paper, and the rest are evaluated by our implementations}
    \label{fig:efficient}
\end{figure*}

\begin{table}[t!]
\caption{Comparison with other methods on BSDS500 dataset. $^\ddagger$ indicates the speeds with our implementations based on a NVIDIA RTX 2080 Ti GPU. $^\dagger$ indicates the cited GPU speeds.}
%\vspace{-0.1in}
\begin{center}
\setlength{\tabcolsep}{0.04\linewidth}
\resizebox*{0.82\linewidth}{!}{
\begin{tabular}{l|c|c|c}
\toprule[1pt]
Method & ODS & OIS & FPS \\
\hline
Human & .803 & .803 & \\
\hline
Canny~\cite{canny1986computational} & .611 & .676 & 28 \\
Pb~\cite{martin2004pb} & .672 & .695 & - \\
SCG~\cite{xiaofeng2012scg} & .739 & .758 & - \\
SE~\cite{dollar2014se} & .743 & .763 & 12.5 \\
OEF~\cite{hallman2015oef} & .746 & .770 & 2/3 \\
\hline
DeepEdge~\cite{bertasius2015deepedge} & .753 & .772 & 1/1000$^\dagger$ \\
DeepContour~\cite{shen2015deepcontour} & .757 & .776 & 1/30$^\dagger$ \\
HFL~\cite{bertasius2015hfl} & .767 & .788 & 5/6$^\dagger$ \\
CEDN~\cite{yang2016cedn} & .788 & .804 & 10$^\dagger$ \\
HED~\cite{xie2017holistically} & .788 & .808 & 78$^\ddagger$ \\
DeepBoundary~\cite{kokkinos2015deepboundary} & .789 & .811 & -\\
COB~\cite{maninis2016cob} & .793 & .820 & - \\
CED~\cite{wang2017ced} & .794 & .811 & - \\
AMH-Net~\cite{xu2018amhnet} & .798 & .829 & - \\
RCF~\cite{liu2019richer} & .806 & .823 & 67$^\ddagger$ \\
LPCB~\cite{deng2018lpcb} & .808 & .824 & 30$^\dagger$ \\
BDCN~\cite{he2019bidirectional} & .820 & .838 & 47$^\ddagger$ \\
\hline
FINED-Inf~\cite{wibisono2020fined} & .788 & .804 & 124$^\dagger$ \\
FINED-Train~\cite{wibisono2020fined} & .790 & .808 & 99$^\dagger$ \\
\hline
Baseline & .798 & .816 & 96$^{\ddagger}$ \\
PiDiNet & .807 & .823 & 92$^{\ddagger,\ast}$ \\
PiDiNet-L & .800 & .815 & 128$^\ddagger$ \\
PiDiNet-Small & .798 & .814 & 148$^\ddagger$ \\
PiDiNet-Small-L & .793 & .809 & 212$^\ddagger$ \\
PiDiNet-Tiny & .789 & .806 & 152$^\ddagger$ \\
PiDiNet-Tiny-L & .787 & .804 & 215$^\ddagger$ \\
\bottomrule[1pt]
\multicolumn{4}{l}{\footnotesize{$^\ast$ ``PiDiNet'' is slightly slower than ``Baseline'' because RPDC is}}\\
\multicolumn{4}{l}{\footnotesize{a 5x5 convolution after conversion.}}
\end{tabular}
}
\end{center}
\label{table:bsds}
\vspace{-0.2in}
\end{table}

\subsection{Network Scalability}
\label{sec:scalability}
PiDiNet is highly compact with only 710K parameters and support training from scratch with limited training data. Here, we explore the scalability of PiDiNet with different model complexities as shown in Fig.~\ref{fig:scalability}. In order to compare with other approaches, the models are trained in two schemes, both use the BSDS500 training and validation set, while with or without mixing the VOC dataset during training. Metrics are recorded on BSDS500 test set. As expected, compared with the basic PiDiNet, smaller models suffer from lower network capacity and thus with degenerated performances in terms of both ODS and OIS scores. At the same time, training with more data constantly leads to higher accuracy. It is noted that the normal scale PiDiNet, can achieve the ODS and OIS scores at the same level as that recorded in the HED approach~\cite{xie2017holistically}, even when trained from scratch only using the BSDS500 dataset (\emph{i.e.}, 0.789 vs. 0.788 in ODS and 0.803 vs. 0.808 in OIS for PiDiNet vs. HED). However, with limited training data, widening the PiDiNet architecture may cause the overfitting problem, as shown in the declines in the second half of the curves. In the following experiments, we only use the tiny, small, and normal versions of PiDiNet, dubbed as PiDiNet-Tiny, PiDiNet-Small and PiDiNet respectively.




\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/bsds_pr.pdf}
    \caption{Precision-Recall curves of our models and some competitors on BSDS500 dataset.}
    \label{fig:bsds_pr}
\end{figure}



\subsection{Comparison with State-of-the-arts}

\vspace{0.3em}
\noindent \textbf{On BSDS500 dataset.} \quad We compare our methods with prior edge detection approaches including both traditional ones and recently proposed CNN based ones, as summarized in Table~\ref{table:bsds} and Fig.~\ref{fig:bsds_pr}. Firstly, we notice that our baseline model can even achieve comparable results, \emph{i.e.}, with ODS of 0.798 and OIS of 0.816, already beating most CNN based models like CED~\cite{wang2017ced}, DeepBoundary~\cite{kokkinos2015deepboundary} and HED~\cite{xie2017holistically}. With PDC, PiDiNet can further boost the performance with ODS of 0.807, being the same level as the recently proposed RCF~\cite{liu2019richer} while still achieving nearly 100 FPS. The fastest version PiDiNet-Tiny-L, can also achieve comparable prediction performance with more than 200 FPS, further demonstrating the effectiveness of our methods. Noting all of our modes are trained from scratch using the same amount of training data as in RCF, LPCB, BDCN, \emph{etc}. (\emph{i.e.}, the training and validation set, mixed with the VOC dataset), without the ImageNet pretraining. We also show some qualitative results in Figure~\ref{fig:bsds_visualization}. A more detailed comparison in terms of network complexity, running efficiency and accuracy can be seen in Fig.~\ref{fig:efficient}. 

\vspace{0.3em}
\noindent \textbf{On NYUD dataset.} \quad The comparison results on the NYUD dataset are illustrated on Table~\ref{table:nyud}. Following the prior works, we get the `RGB-HHA' results by averaging the output edge maps from RGB image and HHA image to get the final edge map. The quantitative comparison shows that PiDiNets can still achieve highly comparable results among the state-of-the-art methods while being efficient. Please refer to the appendix for the Precision-Recall curves.

\vspace{0.3em}
\noindent \textbf{On Multicue dataset.} \quad We also record the evaluation results on Multicue dataset and the comparison results with other methods are shown on Table~\ref{table:multicue}. Still, PiDiNets achieve promising results with high efficiencies. 


%The comparisons with prior state-of-the-art approaches on the three datasets are shown in Table~\ref{table:bsds}, Table~\ref{table:nyud} and Table~\ref{table:multicue}. For BSDS500, we also compare our models with prior works in terms of network complexity, running efficiency and prediction accuracy, as shown in Fig.~\ref{fig:efficient}. The Precision-Recall curves on BSDS500 dataset is shown in Fig.~\ref{fig:bsds_pr} (please refer to the appendix for PR curves on NYUD datasets). Some qualitative results on BSDS500 dataset are illustrated in Fig.~\ref{fig:bsds_visualization}. We get the following conclusions.


\begin{table}[t!]
\caption{Comparison with other methods on NYUD dataset. $^\ddagger$ indicates the speeds with our implementations based on a NVIDIA RTX 2080 Ti GPU.}
\vspace{-0.1in}
\begin{center}
\setlength{\tabcolsep}{0.025\linewidth}
\resizebox*{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c}
% & \multicolumn{2}{c|}{RGB} & \multicolumn{2}{c|}{HHA} & \multicolumn{2}{c|}{RGB-HHA} & FPS \\
%\hhline{~------~}
\toprule[1pt]
Methods & ODS & OIS & ODS & OIS & ODS & OIS & FPS \\
\hline 
gPb-UCM~\cite{arbelaez2010bsds} & .632 & .661 & & & & & 1/360 \\
%OEF~\cite{hallman2015oef} & .651 & .667 & & & & & 1/2 \\
gPb+NG~\cite{gupta2013gpbng} & .687 & .716 & & & & & 1/375 \\
SE~\cite{dollar2014se} & .695 & .708 & & & & & 5 \\
SE+NG+~\cite{gupta2014seng} & .710 & .723 & & & & & 1/15 \\
\hline
\hline
& \multicolumn{2}{c|}{RGB} & \multicolumn{2}{c|}{HHA} & \multicolumn{2}{c|}{RGB-HHA} & \\
\hline
HED~\cite{xie2017holistically} & .720 & .734 & .682 & .695 & .746 & .761 & 62$^\ddagger$ \\
%RCF~\cite{liu2019richer} & .729 & .742 & .705 & .715 & .757 & .771 & 52$^\ddagger$ \\
LPCB~\cite{deng2018lpcb} & .739 & .754 & .707 & .719 & .762 & .778 & - \\
RCF~\cite{liu2019richer} & .743 & .757 & .703 & .717 & .765 & .780 & 52$^\ddagger$ \\
AMH-Net~\cite{xu2018amhnet} & .744 & .758 & .716 & .729 & .771 & .786 & - \\
BDCN~\cite{he2019bidirectional} & .748 & .763 & .707 & .719 & .765 & .781 & 33$^\ddagger$ \\
%COB~\cite{maninis2016cob} & & & & & .784 & .805 & - \\
\hline
PiDiNet & .733 & .747 & .715 & .728 & . 756 & .773 & 62$^\ddagger$ \\
PiDiNet-L & .728 & .741 & .709 & .722 & .754 & .770 & 88$^\ddagger$ \\
PiDiNet-Small & .726 & .741 & .705 & .719 & .750 & .767 & 115$^\ddagger$ \\
PiDiNet-Small-L & .721 & .736 & .701 & .713 & .746 & .763 & 165$^\ddagger$ \\
PiDiNet-Tiny & .721 & .736 & .700 & .714 & .745 & .763 & 140$^\ddagger$ \\
PiDiNet-Tiny-L & .714 & .729 & .693 & .706 & .741 & .759 & 206$^\ddagger$ \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\label{table:nyud}
\vspace{-0.1in}
\end{table}



%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=0.9\linewidth]{images/nyud_pr.pdf}
%    \caption{Precision-Recall curves of our models and some competitors on NYUD dataset.}
%    \label{fig:nyud_pr}
%\end{figure}

\begin{table}[t!]
\caption{Comparison with other methods on Multicue dataset. $^\ddagger$ indicates the speeds with our implementations based on a NVIDIA RTX 2080 Ti GPU.}
\vspace{0.05in}
\begin{center}
\setlength{\tabcolsep}{0.015\linewidth}
\resizebox*{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
\toprule[1pt]
Method & \multicolumn{2}{c|}{Boundary} & \multicolumn{2}{c|}{Edge} & FPS \\
\hhline{~----~}
 & ODS & OIS & ODS & OIS & \\
\hline 
Human~\cite{mely2016multicue} & .760 (.017) & & .750 (.024) &  & \\
\hline
Multicue~\cite{mely2016multicue} & .720 (.014) & & .830 (.002) & & - \\
HED~\cite{xie2017holistically} & .814 (.011) & .822 (.008) & .851 (.014) & .864 (.011) & 18$^\ddagger$ \\
RCF~\cite{liu2019richer} & .817 (.004) & .825 (.005) & .857 (.004) & .862 (.004) & 15$^\ddagger$ \\
BDCN~\cite{he2019bidirectional} & .836 (.001) & .846 (.003) & .891 (.001) & .898 (.002) & 9$^\ddagger$ \\
\hline
PiDiNet & .818 (.003) & .830 (.005) & .855 (.007) & .860 (.005) & 17$^\ddagger$ \\
PiDiNet-L & .810 (.005) & .822 (.002) & .854 (.007) & .860 (.004) & 23$^\ddagger$ \\
PiDiNet-Small & .812 (.004) & .825 (.004) & .858 (.007) & .863 (.004) & 31$^\ddagger$ \\
PiDiNet-Small-L & .805 (.007) & .818 (.002) & .854 (.007) & .860 (.004) & 44$^\ddagger$ \\
PiDiNet-Tiny & .807 (.007) & .819 (.004) & .856 (.006) & .862 (.003) & 43$^\ddagger$ \\
PiDiNet-Tiny-L & .798 (.007) & .811 (.005) & .854 (.008) & .861 (.004) & 56$^\ddagger$ \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\label{table:multicue}
%\vspace{-0.3in}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/vis3.pdf}
    \caption{A qualitative comparison of network outputs with some other methods, including RCF~\cite{liu2019richer}, CED~\cite{wang2017ced} and BDCN~\cite{he2019bidirectional}.}
    \label{fig:bsds_visualization}
\end{figure}


\section{Conclusion}
In conclusion, the contribution in this paper is three-fold:
Firstly, we derive the pixel difference convolution which integrates the wisdom from the traditional edge detectors and the advantages of the deep CNNs, leading to robust and accurate edge detection.
Secondly, we propose a highly efficient architecture named PiDiNet based on pixel difference convolution, which are memory friendly and with high inference speed. Furthermore, PiDiNet can be trained from scratch only using limited data samples, while achieving human-level performances, breaking the convention that high performance CNN based edge detectors usually need a backbone pretrained on large scale dataset.
%(\emph{e.g.}, ImageNet~\cite{deng2009imagenet}).
Thirdly, we conduct extensive experiments on BSDS500, NYUD, and Multicue datasets for edge detection. We believe that PiDiNet has created new state-of-the-art performances considering both accuracy and efficiency.
%(see Fig.~\ref{fig:efficient}). 

\vspace{0.5em}
\noindent \textbf{Future Work.} \quad 
As discussed in Section~\ref{sec:intro}, edge detection is a low level task for many mid- or high-level vision tasks like semantic segmentation and object detection. Also, some low level tasks like salient object detection may also benefit from the image boundary information. We hope pixel difference convolution and the proposed PiDiNet can go further and be useful in these related tasks. 

\vspace{0.5em}
\noindent \textbf{Acknowledgement.} \quad 
This work was partially supported by the Academy of Finland under grant 331883 and the National Natural Science Foundation of China under Grant 61872379, 62022091, and 71701205. The authors also wish to acknowledge CSC IT Center for Science, Finland, for computational resources.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\clearpage

\section{Appendix}

\subsection{Converting Pixel Difference Convolution (PDC) to Vanilla Convolution}

The main goal of the conversion is to make PDC as fast and memory efficient as as the vanilla convolution. 
As introduced in the main paper, the formulations of vanilla convolution and PDC can be written as:

\begin{align}
    y &= f(\pmb{x}, \pmb{\theta}) = \sum_{i=1}^{k\times k}w_{i}\cdot x_{i}, \;\;\;\;\;\;\; \text{(vanilla convolution)}\label{eq:vanilla} \\
    y &= f(\triangledown\pmb{x}, \pmb{\theta}) = \sum_{(x_i, x_i')\in \pmb{\mathcal{P}}}w_{i}\cdot (x_i - x_i'), \;\;\;\;\;\;\, \text{(PDC)} \label{eq: pdc}
\end{align}
where, $x_i$ and $x_i'$ are the pixels in the current input local patch, $w_i$ is the weight in the $k \times k$ convolution kernel. $\pmb{\mathcal{P}} = \{(x_1, x_1'), (x_2, x_2'), ..., (x_m, x_m')\}$ is the set of pixel pairs picked from the local patch, and $m\le k\times k$. 

The conversion from PDC to vanilla convolution can be done in both the training and inference phases.

\vspace{0.3em}
\noindent  \textbf{Conversion in the Training Phase.} \quad Eq.~\ref{eq: pdc} can be transformed to fit the form of Eq.~\ref{eq:vanilla}, according to the selection strategies of the pixel pairs. Correspondingly, PDC can be converted to vanilla convolution by firstly transforming the kernel weights to a new set of kernel weights, followed by a vanilla convolutional operation. We will discuss Central PDC (CPDC), Angular PDC (APDC) and Radial PDC (RPDC) respectively.
The selection strategies of pixel pairs in the three PDC instances are shown in Fig.~\ref{fig:cpdc}, Fig.~\ref{fig:apdc} and Fig.~\ref{fig:rpdc}. The transformations of the equations are as follows.

\vspace{0.3em}
\noindent For CPDC (Fig.~\ref{fig:cpdc}):

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{images/supplement_cpdc.pdf}
    \caption{Selection of pixel pairs and convolution in CPDC.}
    \label{fig:cpdc}
\end{figure}


{\small 
\begin{align}
    y =& w_{1}\cdot (x_1 - x_5) + w_2\cdot (x_2 - x_5)+w_3\cdot (x_3 - x_5)\nonumber\\
    & + w_4\cdot (x_4-x_5) + w_6\cdot (x_6 - x_5) + w_7\cdot (x_7 - x_5)\nonumber \\
    & + w_8\cdot (x_8 - x_5) + w_9\cdot (x_9 - x_5)\nonumber \\
    =&w_1\cdot x_1 + w_2\cdot x_2 + w_3\cdot x_3 +\nonumber \\
    & + w_4\cdot x_4 + w_6\cdot x_6 + w_7\cdot x_7 +\nonumber \\
    & + w_8\cdot x_8 + w_9\cdot x_9\nonumber \\
    & + (-\sum_{i=\{1,2,3,4,6,7,8,9\}}w_i)\cdot x5\nonumber \\
    =&\hat{w}_1\cdot x_1 + \hat{w}_2\cdot x_2 + \hat{w}_3\cdot x_3 + ... =\sum \hat{w}_i\cdot x_i
\end{align}
}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{images/supplement_apdc.pdf}
    \caption{Selection of pixel pairs and convolution in APDC.}
    \label{fig:apdc}
\end{figure}

\vspace{0.3em}
\noindent For APDC (Fig.~\ref{fig:apdc}):

{\small
\begin{align}
    y =& w_{1}\cdot (x_1 - x_2) + w_2\cdot (x_2 - x_3)+w_3\cdot (x_3 - x_6)\nonumber\\
    & + w_4\cdot (x_4-x_1) + w_6\cdot (x_6 - x_9) + w_7\cdot (x_7 - x_4)\nonumber \\
    & + w_8\cdot (x_8 - x_7) + w_9\cdot (x_9 - x_8)\nonumber \\
    =& (w_1 - w_4)\cdot x_1 + (w_2 - w_1)\cdot x_2 + (w_3-w_2)\cdot x_3 \nonumber \\
    & + (w_4 - w_7)\cdot x_4 + (w_6 - w_3)\cdot x_6 + (w_7 - w_8)\cdot x_7 \nonumber \\
    & + (w_8 - w_9)\cdot x_8 + (w_9 - x_6)\cdot x_9\nonumber \\
    & + 0\cdot x_5\nonumber \\
    =& \hat{w}_1\cdot x_1 + \hat{w}_2\cdot x_2 + \hat{w}_3\cdot x_3 + ... =\sum \hat{w}_i\cdot x_i
\end{align}
}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{images/supplement_rpdc.pdf}
    \caption{Selection of pixel pairs and convolution in RPDC.}
    \label{fig:rpdc}
\end{figure}

\vspace{0.3em}
\noindent For RPDC (Fig.~\ref{fig:rpdc}):

{\small
\begin{align}
    y =& w_{1}\cdot (x_1 - x_7) + w_3\cdot (x_3 - x_8)+w_5\cdot (x_5 - x_9)\nonumber\\
    & + w_{11}\cdot (x_{11}-x_{12}) + w_{15}\cdot (x_{15} - x_{14})\nonumber \\
    &  + w_{21}\cdot (x_{21} - x_{17}) + w_{23}\cdot (x_{23} - x_{18})\nonumber \\
    &  + w_{25}\cdot (x_{25} - x_{19})\nonumber\\
    =&w_1\cdot x_1 + w_3\cdot x_3 + w_5\cdot x_5\nonumber \\
    & + (-w_1)\cdot x_7 + (-w_3)\cdot x_8 + (-w_5)\cdot x_9 +\nonumber \\
    & + w_{11}\cdot x_{11} + (-w_{11})\cdot x_{12} + (-w_{15})\cdot x_{14}\nonumber\\
    & + w_{15}\cdot x_{15} + (-w_{21})\cdot x_{17} + (-w_{23})\cdot x_{18}\nonumber\\
    & + (-w_{25})\cdot x_{19} + w_{21}\cdot x_{21} + w_{23}\cdot x_{23}\nonumber\\
    & + w_{25}\cdot x_{25} + \sum_{i=\{2,4,6,10,13,16,20,22,24\}}0\cdot x_i\nonumber\\
    =&\hat{w}_1\cdot x_1 + \hat{w}_2\cdot x_2 + \hat{w}_3\cdot x_3 + ... =\sum \hat{w}_i\cdot x_i
\end{align}
}

The RPDC is converted to a vanilla convolution with kernel size $5\times 5$.

\vspace{0.3em}
\noindent  \textbf{Conversion in the Inference Phase.} \quad After training, instead of saving the original weights $w_i$, we directly save the new set of weights $\hat{w}_i$. Therefore, during inference, all the convolutional operations are vanilla convolutions.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/nyud_pr2.pdf}
    \caption{Precision-Recall curves of our models and some competitors on NYUD dataset.}
    \label{fig:nyud_pr}
\end{figure}

\subsection{Precision-Recall Curves on NYUD Dataset}

The Precision-Reall curves of our methods and other approaches on NYUD dataset~\cite{shi2000nyud} are shown in Fig.~\ref{fig:nyud_pr}. The compared methods include RCF~\cite{liu2019richer}, SE~\cite{dollar2014se}, gPb+NG~\cite{gupta2013gpbng}, gPb-UCM~\cite{arbelaez2010bsds} and OEF~\cite{hallman2015oef}.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.97\linewidth]{images/supplement_fig1_cap.pdf}
    \caption{For each case, Top: input and ground truth image; Middle: edge maps from stage 1, 2, 3, 4 respectively and the final edge map, generated from the baseline architecture, Bottom: Corresponding edge maps generated from PiDiNet. Both the baseline architecture and PiDiNet were trained only using the BSDS500 dataset~\cite{arbelaez2010bsds}. Compared with the baseline, we can see that PiDiNet can detect more useful boundaries (\emph{e.g.}, bangs, stairs, the contour of the tree, the characteristic textures of the car).}
    \label{fig:stages}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/supplement_fig_visual.pdf}
    \caption{CDCM and CSAM can further refine the feature maps with multi-scale feature extraction and the sample adaptive spatial attention mechanism. Note that in the attention maps generated by CSAM, pixels in the background show higher intensities. This makes sense as the background pixels after CDCM have negative values, hence they will be additionally suppressed through CSAM.}
    \label{fig:maps}
\end{figure*}

\subsection{Visualization}

\vspace{0.3em}
\noindent  \textbf{Edge Maps.} \quad The edge maps generated from the baseline architecture and PiDiNet are shown in Fig.~\ref{fig:stages}. Both models were trained using only the BSDS500 dataset without the mixed VOC dataset~\cite{mottaghi2014voc}. From the figure, it is proved that PDC can help PiDiNet effectively capture more useful boundaries, with the ability to extract rich gradient information that facilitates edge detection. 

\vspace{0.3em}
\noindent  \textbf{Intermediate Feature Maps.} \quad We also visualize the intermediate feature maps extracted from PiDiNet, to qualitatively demonstrate the effectiveness of the compact dilation convolution based module (CDCM) and the compact spatial attention module (CSAM), which are shown in Fig.~\ref{fig:maps}. It is concluded that both CDCM and CSAM take a positive role in PiDiNet on the edge detection task.


\end{document}
